slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Job number: 718379
Dataset: truthful
Model: llama-awq
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 0
You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/qa.py:125: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)
  torch.std(torch.stack(rephrasing_gradients), dim=0)
JSONDecodeError: Expecting ',' delimiter: line 20916 column 75 (char 59067)
Skipping sample 237 due to rephrase_text error: Invalid JSON response from API
JSONDecodeError: Expecting ',' delimiter: line 2 column 66 (char 82)
Skipping sample 388 due to rephrase_text error: Invalid JSON response from API
JSONDecodeError: Expecting ',' delimiter: line 5 column 16337 (char 16579)
Skipping sample 538 due to rephrase_text error: Invalid JSON response from API
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/qa.py", line 184, in <module>
    main(args)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/qa.py", line 125, in main
    torch.std(torch.stack(rephrasing_gradients), dim=0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.33 GiB. GPU 0 has a total capacity of 19.71 GiB of which 6.20 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[main e77cfd3] QA Script Results for Run 718379
 2 files changed, 41 insertions(+)
 create mode 100644 slurm-outputs/slurm.718379.abakus11.out
To github.com:ngruenefeld/gradient-uncertainty.git
   8779c53..e77cfd3  main -> main
