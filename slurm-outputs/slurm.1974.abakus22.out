slurmstepd-abakus22: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus22: error: Setting TMPDIR to /tmp
Running job with commit: ff7156f98c6f6d2fe1e2da6dde3c76c2240d861f
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Job number: 1974
Dataset: commoncorpus
Model: polylm-1.7b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 10
Mode: test
Quantization bits: None (full precision)
Full gradient: False
Response only: True
Normalize: False
Perturbation mode: rephrase
Number of perturbations: 3
Using slow tokenizer implementation for polylm-1.7b
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/multiling.py", line 424, in <module>
    main(args)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/multiling.py", line 134, in main
    tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_params)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 921, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 1666, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 1654, in requires_backends
    raise ImportError("".join(failed))
ImportError: 
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

[main d6289f9] Multilingual Script Results for Run 1974 (Commit: ff7156f)
 5 files changed, 674 insertions(+)
 create mode 100644 slurm-outputs/slurm.1974.abakus22.out
To github.com:ngruenefeld/gradient-uncertainty.git
   ff7156f..d6289f9  main -> main
