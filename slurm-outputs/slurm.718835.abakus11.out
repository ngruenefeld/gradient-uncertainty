slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: f046e72212a41f1d10df8f5c39b23601e36d3c53
Job number: 718835
Dataset: trivia
Model: deepseek-r1-distill-qwen-1.5b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 5
Processing sample 1/5 (dataset index: 6181)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.62 GiB is free. Including non-PyTorch memory, this process has 14.06 GiB memory in use. Of the allocated memory 13.34 GiB is allocated by PyTorch, and 507.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 1 (index 6181): CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.62 GiB is free. Including non-PyTorch memory, this process has 14.06 GiB memory in use. Of the allocated memory 13.34 GiB is allocated by PyTorch, and 507.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 2/5 (dataset index: 4484)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.59 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 556.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 2 (index 4484): CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.59 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 556.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 3/5 (dataset index: 473)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.55 GiB is free. Including non-PyTorch memory, this process has 14.13 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 595.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 3 (index 473): CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.55 GiB is free. Including non-PyTorch memory, this process has 14.13 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 595.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 4/5 (dataset index: 2722)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.57 GiB is free. Including non-PyTorch memory, this process has 14.12 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 575.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 4 (index 2722): CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.57 GiB is free. Including non-PyTorch memory, this process has 14.12 GiB memory in use. Of the allocated memory 13.33 GiB is allocated by PyTorch, and 575.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 5/5 (dataset index: 3726)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.59 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 13.31 GiB is allocated by PyTorch, and 563.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 5 (index 3726): CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacity of 19.71 GiB of which 5.59 GiB is free. Including non-PyTorch memory, this process has 14.09 GiB memory in use. Of the allocated memory 13.31 GiB is allocated by PyTorch, and 563.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing complete, but no successful results to save. All 5 samples failed.
