slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: 1d57233fb79a39d209e72abbfa133953e2e3bfe0
Running command: python -um scripts.bert "59927" --key_mode "keyfile" --sample_size "0" --test_sample_size "5" --dataset "scienceqa" --epochs "10"
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Job number: 59927
Key mode: keyfile
Sample size: 0
Normalize: False
Counterfactual: identity
Dataset: scienceqa
Model: bert
Replacement probability: 1.0
Epochs: 10
Using full train dataset with 547 samples.
Using 5 randomly sampled examples from the test dataset.
Loading model: bert-base-uncased
BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using device: cuda
Map:   0%|          | 0/547 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 547/547 [00:00<00:00, 8482.14 examples/s]
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/bert.py:240: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Calculating uncertainties for 5 test samples before training (counterfactual mode: identity)...
Processing test sample 1/5 (before training)
MaskedLMOutput(loss=tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ -8.9589,  -9.2124,  -9.0087,  ...,  -8.5620,  -8.2521,  -5.5923],
         [ -9.2225,  -9.2713,  -9.2903,  ...,  -8.2428,  -7.8086,  -5.5038],
         [-12.1527, -12.3131, -12.2831,  ..., -10.9891, -10.6337, -10.4784],
         ...,
         [ -6.5632,  -6.6905,  -6.4040,  ...,  -5.4808,  -6.2877,  -5.8993],
         [-10.2183, -10.4788, -10.1747,  ...,  -9.3837,  -8.5009, -11.7229],
         [-15.3626, -15.4342, -15.4131,  ..., -14.4993, -13.0231, -12.4271]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 118, 30522])
tensor([[[ -8.9589,  -9.2124,  -9.0087,  ...,  -8.5620,  -8.2521,  -5.5923],
         [ -9.2225,  -9.2713,  -9.2903,  ...,  -8.2428,  -7.8086,  -5.5038],
         [-12.1527, -12.3131, -12.2831,  ..., -10.9891, -10.6337, -10.4784],
         ...,
         [ -6.5632,  -6.6905,  -6.4040,  ...,  -5.4808,  -6.2877,  -5.8993],
         [-10.2183, -10.4788, -10.1747,  ...,  -9.3837,  -8.5009, -11.7229],
         [-15.3626, -15.4342, -15.4131,  ..., -14.4993, -13.0231, -12.4271]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
torch.Size([118, 30522])
tensor([[ -8.9589,  -9.2124,  -9.0087,  ...,  -8.5620,  -8.2521,  -5.5923],
        [ -9.2225,  -9.2713,  -9.2903,  ...,  -8.2428,  -7.8086,  -5.5038],
        [-12.1527, -12.3131, -12.2831,  ..., -10.9891, -10.6337, -10.4784],
        ...,
        [ -6.5632,  -6.6905,  -6.4040,  ...,  -5.4808,  -6.2877,  -5.8993],
        [-10.2183, -10.4788, -10.1747,  ...,  -9.3837,  -8.5009, -11.7229],
        [-15.3626, -15.4342, -15.4131,  ..., -14.4993, -13.0231, -12.4271]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([118, 30522])
tensor([[ -8.9589,  -9.2124,  -9.0087,  ...,  -8.5620,  -8.2521,  -5.5923],
        [ -9.2225,  -9.2713,  -9.2903,  ...,  -8.2428,  -7.8086,  -5.5038],
        [-12.1527, -12.3131, -12.2831,  ..., -10.9891, -10.6337, -10.4784],
        ...,
        [ -6.5632,  -6.6905,  -6.4040,  ...,  -5.4808,  -6.2877,  -5.8993],
        [-10.2183, -10.4788, -10.1747,  ...,  -9.3837,  -8.5009, -11.7229],
        [-15.3626, -15.4342, -15.4131,  ..., -14.4993, -13.0231, -12.4271]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 2/5 (before training)
MaskedLMOutput(loss=tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ -8.2355,  -8.5921,  -8.4578,  ...,  -8.2222,  -7.1478,  -4.8939],
         [ -8.6431,  -9.2280,  -8.9742,  ...,  -8.5927,  -7.1466,  -5.4276],
         [ -9.9752, -10.1975,  -9.9550,  ...,  -8.2747,  -8.4151,  -4.3175],
         ...,
         [-11.7323, -11.7291, -11.8369,  ..., -10.2224,  -9.0872,  -4.3519],
         [-12.7622, -13.0425, -13.0382,  ..., -11.1565,  -9.5494,  -3.5433],
         [-14.3238, -14.5633, -14.6095,  ..., -13.4067, -11.7448, -10.8818]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 122, 30522])
tensor([[[ -8.2355,  -8.5921,  -8.4578,  ...,  -8.2222,  -7.1478,  -4.8939],
         [ -8.6431,  -9.2280,  -8.9742,  ...,  -8.5927,  -7.1466,  -5.4276],
         [ -9.9752, -10.1975,  -9.9550,  ...,  -8.2747,  -8.4151,  -4.3175],
         ...,
         [-11.7323, -11.7291, -11.8369,  ..., -10.2224,  -9.0872,  -4.3519],
         [-12.7622, -13.0425, -13.0382,  ..., -11.1565,  -9.5494,  -3.5433],
         [-14.3238, -14.5633, -14.6095,  ..., -13.4067, -11.7448, -10.8818]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
torch.Size([122, 30522])
tensor([[ -8.2355,  -8.5921,  -8.4578,  ...,  -8.2222,  -7.1478,  -4.8939],
        [ -8.6431,  -9.2280,  -8.9742,  ...,  -8.5927,  -7.1466,  -5.4276],
        [ -9.9752, -10.1975,  -9.9550,  ...,  -8.2747,  -8.4151,  -4.3175],
        ...,
        [-11.7323, -11.7291, -11.8369,  ..., -10.2224,  -9.0872,  -4.3519],
        [-12.7622, -13.0425, -13.0382,  ..., -11.1565,  -9.5494,  -3.5433],
        [-14.3238, -14.5633, -14.6095,  ..., -13.4067, -11.7448, -10.8818]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([122, 30522])
tensor([[ -8.2355,  -8.5921,  -8.4578,  ...,  -8.2222,  -7.1478,  -4.8939],
        [ -8.6431,  -9.2280,  -8.9742,  ...,  -8.5927,  -7.1466,  -5.4276],
        [ -9.9752, -10.1975,  -9.9550,  ...,  -8.2747,  -8.4151,  -4.3175],
        ...,
        [-11.7323, -11.7291, -11.8369,  ..., -10.2224,  -9.0872,  -4.3519],
        [-12.7622, -13.0425, -13.0382,  ..., -11.1565,  -9.5494,  -3.5433],
        [-14.3238, -14.5633, -14.6095,  ..., -13.4067, -11.7448, -10.8818]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 3/5 (before training)
MaskedLMOutput(loss=tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-11.0019, -11.0078, -10.6681,  ..., -10.4277,  -9.1158,  -6.8580],
         [-13.1075, -13.2691, -13.2419,  ..., -11.6689, -10.2038,  -9.2441],
         [-11.6449, -11.7479, -11.5268,  ...,  -9.8711,  -8.3962, -10.1772],
         ...,
         [-15.5543, -15.9943, -15.5035,  ..., -14.6932, -11.8720,  -9.9195],
         [ -9.6992,  -9.7424,  -9.7506,  ...,  -8.7365,  -7.4304,  -5.1601],
         [-13.9712, -13.9416, -13.9745,  ..., -12.6631, -10.9146,  -8.6500]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 58, 30522])
tensor([[[-11.0019, -11.0078, -10.6681,  ..., -10.4277,  -9.1158,  -6.8580],
         [-13.1075, -13.2691, -13.2419,  ..., -11.6689, -10.2038,  -9.2441],
         [-11.6449, -11.7479, -11.5268,  ...,  -9.8711,  -8.3962, -10.1772],
         ...,
         [-15.5543, -15.9943, -15.5035,  ..., -14.6932, -11.8720,  -9.9195],
         [ -9.6992,  -9.7424,  -9.7506,  ...,  -8.7365,  -7.4304,  -5.1601],
         [-13.9712, -13.9416, -13.9745,  ..., -12.6631, -10.9146,  -8.6500]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
torch.Size([58, 30522])
tensor([[-11.0019, -11.0078, -10.6681,  ..., -10.4277,  -9.1158,  -6.8580],
        [-13.1075, -13.2691, -13.2419,  ..., -11.6689, -10.2038,  -9.2441],
        [-11.6449, -11.7479, -11.5268,  ...,  -9.8711,  -8.3962, -10.1772],
        ...,
        [-15.5543, -15.9943, -15.5035,  ..., -14.6932, -11.8720,  -9.9195],
        [ -9.6992,  -9.7424,  -9.7506,  ...,  -8.7365,  -7.4304,  -5.1601],
        [-13.9712, -13.9416, -13.9745,  ..., -12.6631, -10.9146,  -8.6500]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([58, 30522])
tensor([[-11.0019, -11.0078, -10.6681,  ..., -10.4277,  -9.1158,  -6.8580],
        [-13.1075, -13.2691, -13.2419,  ..., -11.6689, -10.2038,  -9.2441],
        [-11.6449, -11.7479, -11.5268,  ...,  -9.8711,  -8.3962, -10.1772],
        ...,
        [-15.5543, -15.9943, -15.5035,  ..., -14.6932, -11.8720,  -9.9195],
        [ -9.6992,  -9.7424,  -9.7506,  ...,  -8.7365,  -7.4304,  -5.1601],
        [-13.9712, -13.9416, -13.9745,  ..., -12.6631, -10.9146,  -8.6500]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 4/5 (before training)
MaskedLMOutput(loss=tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ -7.2089,  -7.3905,  -7.1640,  ...,  -7.3970,  -6.6085,  -4.1289],
         [ -8.4980,  -8.8756,  -8.8081,  ...,  -6.6978,  -6.9450,  -7.7002],
         [-15.5321, -15.2314, -15.6146,  ..., -11.5135, -10.5548, -11.6098],
         ...,
         [ -3.6753,  -3.6796,  -4.0480,  ...,  -4.1495,  -5.2201,   1.6707],
         [ -8.5665,  -8.7744,  -8.8773,  ...,  -7.0178,  -8.4486,  -1.7793],
         [-12.3261, -12.7064, -12.4876,  ..., -12.8283, -11.2173,  -8.7175]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 152, 30522])
tensor([[[ -7.2089,  -7.3905,  -7.1640,  ...,  -7.3970,  -6.6085,  -4.1289],
         [ -8.4980,  -8.8756,  -8.8081,  ...,  -6.6978,  -6.9450,  -7.7002],
         [-15.5321, -15.2314, -15.6146,  ..., -11.5135, -10.5548, -11.6098],
         ...,
         [ -3.6753,  -3.6796,  -4.0480,  ...,  -4.1495,  -5.2201,   1.6707],
         [ -8.5665,  -8.7744,  -8.8773,  ...,  -7.0178,  -8.4486,  -1.7793],
         [-12.3261, -12.7064, -12.4876,  ..., -12.8283, -11.2173,  -8.7175]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
torch.Size([152, 30522])
tensor([[ -7.2089,  -7.3905,  -7.1640,  ...,  -7.3970,  -6.6085,  -4.1289],
        [ -8.4980,  -8.8756,  -8.8081,  ...,  -6.6978,  -6.9450,  -7.7002],
        [-15.5321, -15.2314, -15.6146,  ..., -11.5135, -10.5548, -11.6098],
        ...,
        [ -3.6753,  -3.6796,  -4.0480,  ...,  -4.1495,  -5.2201,   1.6707],
        [ -8.5665,  -8.7744,  -8.8773,  ...,  -7.0178,  -8.4486,  -1.7793],
        [-12.3261, -12.7064, -12.4876,  ..., -12.8283, -11.2173,  -8.7175]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([152, 30522])
tensor([[ -7.2089,  -7.3905,  -7.1640,  ...,  -7.3970,  -6.6085,  -4.1289],
        [ -8.4980,  -8.8756,  -8.8081,  ...,  -6.6978,  -6.9450,  -7.7002],
        [-15.5321, -15.2314, -15.6146,  ..., -11.5135, -10.5548, -11.6098],
        ...,
        [ -3.6753,  -3.6796,  -4.0480,  ...,  -4.1495,  -5.2201,   1.6707],
        [ -8.5665,  -8.7744,  -8.8773,  ...,  -7.0178,  -8.4486,  -1.7793],
        [-12.3261, -12.7064, -12.4876,  ..., -12.8283, -11.2173,  -8.7175]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 5/5 (before training)
MaskedLMOutput(loss=tensor(0.9529, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-12.2281, -12.2368, -12.1634,  ..., -12.3055, -10.7841,  -8.5849],
         [-10.1114,  -9.9365, -10.2566,  ...,  -8.8486,  -9.4013,  -8.7268],
         [-12.8743, -12.7423, -12.8941,  ..., -10.9372,  -9.9079,  -9.6873],
         ...,
         [-13.6579, -14.0965, -14.0408,  ..., -11.6194, -11.7752,  -9.5153],
         [ -9.5230,  -9.5283,  -9.5015,  ...,  -9.6594,  -8.7538,  -9.9618],
         [-15.1988, -15.5292, -15.3071,  ..., -14.9608, -12.8424, -11.9832]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 21, 30522])
tensor([[[-12.2281, -12.2368, -12.1634,  ..., -12.3055, -10.7841,  -8.5849],
         [-10.1114,  -9.9365, -10.2566,  ...,  -8.8486,  -9.4013,  -8.7268],
         [-12.8743, -12.7423, -12.8941,  ..., -10.9372,  -9.9079,  -9.6873],
         ...,
         [-13.6579, -14.0965, -14.0408,  ..., -11.6194, -11.7752,  -9.5153],
         [ -9.5230,  -9.5283,  -9.5015,  ...,  -9.6594,  -8.7538,  -9.9618],
         [-15.1988, -15.5292, -15.3071,  ..., -14.9608, -12.8424, -11.9832]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
torch.Size([21, 30522])
tensor([[-12.2281, -12.2368, -12.1634,  ..., -12.3055, -10.7841,  -8.5849],
        [-10.1114,  -9.9365, -10.2566,  ...,  -8.8486,  -9.4013,  -8.7268],
        [-12.8743, -12.7423, -12.8941,  ..., -10.9372,  -9.9079,  -9.6873],
        ...,
        [-13.6579, -14.0965, -14.0408,  ..., -11.6194, -11.7752,  -9.5153],
        [ -9.5230,  -9.5283,  -9.5015,  ...,  -9.6594,  -8.7538,  -9.9618],
        [-15.1988, -15.5292, -15.3071,  ..., -14.9608, -12.8424, -11.9832]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([21, 30522])
tensor([[-12.2281, -12.2368, -12.1634,  ..., -12.3055, -10.7841,  -8.5849],
        [-10.1114,  -9.9365, -10.2566,  ...,  -8.8486,  -9.4013,  -8.7268],
        [-12.8743, -12.7423, -12.8941,  ..., -10.9372,  -9.9079,  -9.6873],
        ...,
        [-13.6579, -14.0965, -14.0408,  ..., -11.6194, -11.7752,  -9.5153],
        [ -9.5230,  -9.5283,  -9.5015,  ...,  -9.6594,  -8.7538,  -9.9618],
        [-15.1988, -15.5292, -15.3071,  ..., -14.9608, -12.8424, -11.9832]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Training model...
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:07,  5.20it/s]  1%|          | 3/350 [00:00<00:36,  9.60it/s]  1%|â–         | 5/350 [00:00<00:28, 11.90it/s]  2%|â–         | 7/350 [00:00<00:26, 12.85it/s]  3%|â–Ž         | 9/350 [00:00<00:25, 13.40it/s]  3%|â–Ž         | 11/350 [00:00<00:24, 13.75it/s]  4%|â–Ž         | 13/350 [00:01<00:24, 13.99it/s]  4%|â–         | 15/350 [00:01<00:23, 14.14it/s]  5%|â–         | 17/350 [00:01<00:23, 14.24it/s]  5%|â–Œ         | 19/350 [00:01<00:23, 14.30it/s]  6%|â–Œ         | 21/350 [00:01<00:22, 14.33it/s]  7%|â–‹         | 23/350 [00:01<00:22, 14.36it/s]  7%|â–‹         | 25/350 [00:01<00:22, 14.38it/s]  8%|â–Š         | 27/350 [00:01<00:22, 14.40it/s]  8%|â–Š         | 29/350 [00:02<00:22, 14.38it/s]  9%|â–‰         | 31/350 [00:02<00:22, 14.39it/s]  9%|â–‰         | 33/350 [00:02<00:21, 14.41it/s] 10%|â–ˆ         | 35/350 [00:02<00:20, 15.35it/s] 11%|â–ˆ         | 37/350 [00:02<00:20, 15.11it/s] 11%|â–ˆ         | 39/350 [00:02<00:20, 14.88it/s] 12%|â–ˆâ–        | 41/350 [00:02<00:20, 14.73it/s] 12%|â–ˆâ–        | 43/350 [00:03<00:20, 14.63it/s] 13%|â–ˆâ–Ž        | 45/350 [00:03<00:20, 14.57it/s] 13%|â–ˆâ–Ž        | 47/350 [00:03<00:20, 14.49it/s] 14%|â–ˆâ–        | 49/350 [00:03<00:20, 14.47it/s] 15%|â–ˆâ–        | 51/350 [00:03<00:20, 14.46it/s] 15%|â–ˆâ–Œ        | 53/350 [00:03<00:20, 14.44it/s] 16%|â–ˆâ–Œ        | 55/350 [00:03<00:20, 14.43it/s] 16%|â–ˆâ–‹        | 57/350 [00:04<00:20, 14.41it/s] 17%|â–ˆâ–‹        | 59/350 [00:04<00:20, 14.40it/s] 17%|â–ˆâ–‹        | 61/350 [00:04<00:20, 14.41it/s] 18%|â–ˆâ–Š        | 63/350 [00:04<00:19, 14.41it/s] 19%|â–ˆâ–Š        | 65/350 [00:04<00:19, 14.41it/s] 19%|â–ˆâ–‰        | 67/350 [00:04<00:19, 14.42it/s] 20%|â–ˆâ–‰        | 69/350 [00:04<00:19, 14.53it/s] 20%|â–ˆâ–ˆ        | 71/350 [00:04<00:18, 15.40it/s] 21%|â–ˆâ–ˆ        | 73/350 [00:05<00:18, 15.08it/s] 21%|â–ˆâ–ˆâ–       | 75/350 [00:05<00:18, 14.87it/s] 22%|â–ˆâ–ˆâ–       | 77/350 [00:05<00:18, 14.72it/s] 23%|â–ˆâ–ˆâ–Ž       | 79/350 [00:05<00:18, 14.63it/s] 23%|â–ˆâ–ˆâ–Ž       | 81/350 [00:05<00:18, 14.57it/s] 24%|â–ˆâ–ˆâ–Ž       | 83/350 [00:05<00:18, 14.52it/s] 24%|â–ˆâ–ˆâ–       | 85/350 [00:05<00:18, 14.49it/s] 25%|â–ˆâ–ˆâ–       | 87/350 [00:06<00:18, 14.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 89/350 [00:06<00:18, 14.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 91/350 [00:06<00:18, 14.33it/s] 27%|â–ˆâ–ˆâ–‹       | 93/350 [00:06<00:17, 14.30it/s] 27%|â–ˆâ–ˆâ–‹       | 95/350 [00:06<00:17, 14.22it/s] 28%|â–ˆâ–ˆâ–Š       | 97/350 [00:06<00:18, 13.85it/s] 28%|â–ˆâ–ˆâ–Š       | 99/350 [00:06<00:18, 13.59it/s]                                                {'loss': 0.4, 'grad_norm': 5.802941799163818, 'learning_rate': 3.6e-05, 'epoch': 2.86}
 29%|â–ˆâ–ˆâ–Š       | 100/350 [00:07<00:18, 13.59it/s] 29%|â–ˆâ–ˆâ–‰       | 101/350 [00:07<00:18, 13.64it/s] 29%|â–ˆâ–ˆâ–‰       | 103/350 [00:07<00:17, 13.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 105/350 [00:07<00:16, 14.79it/s] 31%|â–ˆâ–ˆâ–ˆ       | 107/350 [00:07<00:16, 14.60it/s] 31%|â–ˆâ–ˆâ–ˆ       | 109/350 [00:07<00:16, 14.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 111/350 [00:07<00:16, 14.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 113/350 [00:07<00:16, 14.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115/350 [00:08<00:16, 14.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117/350 [00:08<00:16, 14.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 119/350 [00:08<00:16, 14.25it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 121/350 [00:08<00:16, 14.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123/350 [00:08<00:15, 14.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125/350 [00:08<00:15, 14.31it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 127/350 [00:08<00:15, 14.31it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 129/350 [00:09<00:15, 14.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 131/350 [00:09<00:15, 14.32it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 133/350 [00:09<00:15, 14.31it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 135/350 [00:09<00:15, 14.31it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 137/350 [00:09<00:14, 14.31it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 139/350 [00:09<00:14, 14.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141/350 [00:09<00:13, 15.37it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143/350 [00:10<00:13, 15.06it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145/350 [00:10<00:13, 14.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147/350 [00:10<00:13, 14.61it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149/350 [00:10<00:13, 14.43it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151/350 [00:10<00:13, 14.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/350 [00:10<00:13, 14.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/350 [00:10<00:13, 14.26it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/350 [00:10<00:13, 14.25it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159/350 [00:11<00:13, 14.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161/350 [00:11<00:13, 14.24it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163/350 [00:11<00:13, 14.13it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165/350 [00:11<00:13, 14.14it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167/350 [00:11<00:12, 14.13it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169/350 [00:11<00:12, 14.14it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171/350 [00:11<00:12, 14.13it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173/350 [00:12<00:12, 14.11it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175/350 [00:12<00:11, 15.10it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177/350 [00:12<00:11, 14.88it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179/350 [00:12<00:11, 14.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181/350 [00:12<00:11, 14.50it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183/350 [00:12<00:11, 14.39it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185/350 [00:12<00:11, 14.33it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187/350 [00:13<00:11, 14.23it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/350 [00:13<00:11, 14.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/350 [00:13<00:11, 14.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193/350 [00:13<00:11, 14.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195/350 [00:13<00:10, 14.20it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197/350 [00:13<00:10, 14.21it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199/350 [00:13<00:10, 14.23it/s]                                                 {'loss': 0.0653, 'grad_norm': 0.42163437604904175, 'learning_rate': 2.1714285714285715e-05, 'epoch': 5.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200/350 [00:14<00:10, 14.23it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201/350 [00:14<00:10, 14.23it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203/350 [00:14<00:10, 14.21it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205/350 [00:14<00:10, 14.20it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207/350 [00:14<00:10, 14.20it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209/350 [00:14<00:09, 14.29it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211/350 [00:14<00:09, 15.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213/350 [00:14<00:09, 14.80it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215/350 [00:15<00:09, 14.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217/350 [00:15<00:09, 14.48it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219/350 [00:15<00:09, 14.37it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221/350 [00:15<00:09, 14.30it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223/350 [00:15<00:08, 14.26it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/350 [00:15<00:08, 14.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/350 [00:15<00:08, 14.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229/350 [00:16<00:08, 14.15it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231/350 [00:16<00:08, 14.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233/350 [00:16<00:08, 14.22it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235/350 [00:16<00:08, 14.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237/350 [00:16<00:07, 14.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239/350 [00:16<00:07, 14.31it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241/350 [00:16<00:07, 14.32it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243/350 [00:16<00:07, 14.33it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245/350 [00:17<00:06, 15.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247/350 [00:17<00:06, 15.14it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249/350 [00:17<00:06, 14.89it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251/350 [00:17<00:06, 14.70it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253/350 [00:17<00:06, 14.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255/350 [00:17<00:06, 14.47it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257/350 [00:17<00:06, 14.39it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259/350 [00:18<00:06, 14.37it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261/350 [00:18<00:06, 14.35it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263/350 [00:18<00:06, 14.35it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265/350 [00:18<00:05, 14.33it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267/350 [00:18<00:05, 14.33it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269/350 [00:18<00:05, 14.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271/350 [00:18<00:05, 14.31it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273/350 [00:19<00:05, 14.32it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275/350 [00:19<00:05, 14.32it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277/350 [00:19<00:05, 14.33it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279/350 [00:19<00:04, 14.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281/350 [00:19<00:04, 15.45it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283/350 [00:19<00:04, 15.11it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285/350 [00:19<00:04, 14.89it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287/350 [00:19<00:04, 14.72it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289/350 [00:20<00:04, 14.60it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291/350 [00:20<00:04, 14.51it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293/350 [00:20<00:03, 14.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295/350 [00:20<00:03, 14.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297/350 [00:20<00:03, 14.38it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299/350 [00:20<00:03, 14.36it/s]                                                 {'loss': 0.0319, 'grad_norm': 3.593410015106201, 'learning_rate': 7.428571428571429e-06, 'epoch': 8.57}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300/350 [00:20<00:03, 14.36it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301/350 [00:20<00:03, 14.33it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303/350 [00:21<00:03, 14.34it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305/350 [00:21<00:03, 14.32it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307/350 [00:21<00:03, 14.33it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309/350 [00:21<00:02, 14.33it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311/350 [00:21<00:02, 14.33it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313/350 [00:21<00:02, 14.33it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315/350 [00:21<00:02, 15.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317/350 [00:22<00:02, 15.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319/350 [00:22<00:02, 14.89it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321/350 [00:22<00:01, 14.71it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323/350 [00:22<00:01, 14.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325/350 [00:22<00:01, 14.52it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327/350 [00:22<00:01, 14.34it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329/350 [00:22<00:01, 14.33it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331/350 [00:23<00:01, 14.33it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333/350 [00:23<00:01, 14.34it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335/350 [00:23<00:01, 14.35it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337/350 [00:23<00:00, 14.34it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339/350 [00:23<00:00, 14.34it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341/350 [00:23<00:00, 14.33it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343/350 [00:23<00:00, 14.32it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345/350 [00:24<00:00, 14.32it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347/350 [00:24<00:00, 14.31it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349/350 [00:24<00:00, 14.40it/s]                                                 {'train_runtime': 24.3428, 'train_samples_per_second': 224.707, 'train_steps_per_second': 14.378, 'train_loss': 0.14734707968575614, 'epoch': 10.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:24<00:00, 14.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:24<00:00, 14.38it/s]
Calculating uncertainties for 5 test samples after training (counterfactual mode: identity)...
Processing test sample 1/5 (after training)
MaskedLMOutput(loss=tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ -9.5078, -10.2891, -10.0078,  ...,  -9.1406, -10.9141,  -6.4102],
         [-12.1797, -12.2734, -12.2500,  ..., -10.8359, -11.4141, -10.7031],
         [-12.1172, -12.4375, -12.2734,  ...,  -9.3906, -10.0156, -13.3672],
         ...,
         [ -7.4414,  -7.8438,  -7.7656,  ...,  -4.9844,  -6.4961,  -8.8750],
         [ -8.8984,  -9.4297,  -9.1016,  ...,  -7.8008,  -8.4375, -10.9453],
         [-17.7031, -17.9688, -18.3750,  ..., -15.8203, -15.5859, -12.4297]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 118, 30522])
tensor([[[ -9.5078, -10.2891, -10.0078,  ...,  -9.1406, -10.9141,  -6.4102],
         [-12.1797, -12.2734, -12.2500,  ..., -10.8359, -11.4141, -10.7031],
         [-12.1172, -12.4375, -12.2734,  ...,  -9.3906, -10.0156, -13.3672],
         ...,
         [ -7.4414,  -7.8438,  -7.7656,  ...,  -4.9844,  -6.4961,  -8.8750],
         [ -8.8984,  -9.4297,  -9.1016,  ...,  -7.8008,  -8.4375, -10.9453],
         [-17.7031, -17.9688, -18.3750,  ..., -15.8203, -15.5859, -12.4297]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
torch.Size([118, 30522])
tensor([[ -9.5078, -10.2891, -10.0078,  ...,  -9.1406, -10.9141,  -6.4102],
        [-12.1797, -12.2734, -12.2500,  ..., -10.8359, -11.4141, -10.7031],
        [-12.1172, -12.4375, -12.2734,  ...,  -9.3906, -10.0156, -13.3672],
        ...,
        [ -7.4414,  -7.8438,  -7.7656,  ...,  -4.9844,  -6.4961,  -8.8750],
        [ -8.8984,  -9.4297,  -9.1016,  ...,  -7.8008,  -8.4375, -10.9453],
        [-17.7031, -17.9688, -18.3750,  ..., -15.8203, -15.5859, -12.4297]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([118, 30522])
tensor([[ -9.5078, -10.2891, -10.0078,  ...,  -9.1406, -10.9141,  -6.4102],
        [-12.1797, -12.2734, -12.2500,  ..., -10.8359, -11.4141, -10.7031],
        [-12.1172, -12.4375, -12.2734,  ...,  -9.3906, -10.0156, -13.3672],
        ...,
        [ -7.4414,  -7.8438,  -7.7656,  ...,  -4.9844,  -6.4961,  -8.8750],
        [ -8.8984,  -9.4297,  -9.1016,  ...,  -7.8008,  -8.4375, -10.9453],
        [-17.7031, -17.9688, -18.3750,  ..., -15.8203, -15.5859, -12.4297]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 2/5 (after training)
MaskedLMOutput(loss=tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-12.4297, -12.8906, -12.8984,  ..., -12.2109, -11.3359,  -6.9336],
         [ -8.6875,  -9.1484,  -8.8359,  ...,  -8.5547,  -8.8359,  -6.2109],
         [ -6.9375,  -7.1562,  -6.8398,  ...,  -5.5938,  -5.9648,  -6.1758],
         ...,
         [-12.9375, -12.8828, -13.3359,  ..., -11.6094, -12.9062,  -8.0547],
         [-11.0000, -11.5703, -11.2344,  ...,  -9.4844, -10.1406,  -5.6641],
         [-13.8047, -13.9453, -13.8594,  ..., -13.3906, -12.7109,  -9.3594]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 122, 30522])
tensor([[[-12.4297, -12.8906, -12.8984,  ..., -12.2109, -11.3359,  -6.9336],
         [ -8.6875,  -9.1484,  -8.8359,  ...,  -8.5547,  -8.8359,  -6.2109],
         [ -6.9375,  -7.1562,  -6.8398,  ...,  -5.5938,  -5.9648,  -6.1758],
         ...,
         [-12.9375, -12.8828, -13.3359,  ..., -11.6094, -12.9062,  -8.0547],
         [-11.0000, -11.5703, -11.2344,  ...,  -9.4844, -10.1406,  -5.6641],
         [-13.8047, -13.9453, -13.8594,  ..., -13.3906, -12.7109,  -9.3594]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
torch.Size([122, 30522])
tensor([[-12.4297, -12.8906, -12.8984,  ..., -12.2109, -11.3359,  -6.9336],
        [ -8.6875,  -9.1484,  -8.8359,  ...,  -8.5547,  -8.8359,  -6.2109],
        [ -6.9375,  -7.1562,  -6.8398,  ...,  -5.5938,  -5.9648,  -6.1758],
        ...,
        [-12.9375, -12.8828, -13.3359,  ..., -11.6094, -12.9062,  -8.0547],
        [-11.0000, -11.5703, -11.2344,  ...,  -9.4844, -10.1406,  -5.6641],
        [-13.8047, -13.9453, -13.8594,  ..., -13.3906, -12.7109,  -9.3594]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([122, 30522])
tensor([[-12.4297, -12.8906, -12.8984,  ..., -12.2109, -11.3359,  -6.9336],
        [ -8.6875,  -9.1484,  -8.8359,  ...,  -8.5547,  -8.8359,  -6.2109],
        [ -6.9375,  -7.1562,  -6.8398,  ...,  -5.5938,  -5.9648,  -6.1758],
        ...,
        [-12.9375, -12.8828, -13.3359,  ..., -11.6094, -12.9062,  -8.0547],
        [-11.0000, -11.5703, -11.2344,  ...,  -9.4844, -10.1406,  -5.6641],
        [-13.8047, -13.9453, -13.8594,  ..., -13.3906, -12.7109,  -9.3594]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 3/5 (after training)
MaskedLMOutput(loss=tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-13.9297, -14.5859, -14.2500,  ..., -14.2031, -13.8359,  -9.0469],
         [-12.5000, -13.0625, -13.0234,  ..., -11.4531, -11.6719,  -8.5703],
         [-13.1953, -13.7500, -13.3906,  ..., -11.5078, -11.8594, -14.1406],
         ...,
         [-13.1875, -13.7188, -13.2266,  ..., -12.5469, -11.9688,  -7.2070],
         [ -9.2578,  -9.5000,  -9.3438,  ...,  -7.4336,  -8.3125,  -7.7578],
         [-13.5078, -13.6641, -13.6172,  ..., -11.7578, -13.4766, -11.0938]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 58, 30522])
tensor([[[-13.9297, -14.5859, -14.2500,  ..., -14.2031, -13.8359,  -9.0469],
         [-12.5000, -13.0625, -13.0234,  ..., -11.4531, -11.6719,  -8.5703],
         [-13.1953, -13.7500, -13.3906,  ..., -11.5078, -11.8594, -14.1406],
         ...,
         [-13.1875, -13.7188, -13.2266,  ..., -12.5469, -11.9688,  -7.2070],
         [ -9.2578,  -9.5000,  -9.3438,  ...,  -7.4336,  -8.3125,  -7.7578],
         [-13.5078, -13.6641, -13.6172,  ..., -11.7578, -13.4766, -11.0938]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
torch.Size([58, 30522])
tensor([[-13.9297, -14.5859, -14.2500,  ..., -14.2031, -13.8359,  -9.0469],
        [-12.5000, -13.0625, -13.0234,  ..., -11.4531, -11.6719,  -8.5703],
        [-13.1953, -13.7500, -13.3906,  ..., -11.5078, -11.8594, -14.1406],
        ...,
        [-13.1875, -13.7188, -13.2266,  ..., -12.5469, -11.9688,  -7.2070],
        [ -9.2578,  -9.5000,  -9.3438,  ...,  -7.4336,  -8.3125,  -7.7578],
        [-13.5078, -13.6641, -13.6172,  ..., -11.7578, -13.4766, -11.0938]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([58, 30522])
tensor([[-13.9297, -14.5859, -14.2500,  ..., -14.2031, -13.8359,  -9.0469],
        [-12.5000, -13.0625, -13.0234,  ..., -11.4531, -11.6719,  -8.5703],
        [-13.1953, -13.7500, -13.3906,  ..., -11.5078, -11.8594, -14.1406],
        ...,
        [-13.1875, -13.7188, -13.2266,  ..., -12.5469, -11.9688,  -7.2070],
        [ -9.2578,  -9.5000,  -9.3438,  ...,  -7.4336,  -8.3125,  -7.7578],
        [-13.5078, -13.6641, -13.6172,  ..., -11.7578, -13.4766, -11.0938]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 4/5 (after training)
MaskedLMOutput(loss=tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-11.1406, -11.7344, -11.5625,  ..., -12.0859, -12.2344,  -9.1016],
         [-11.0703, -11.7188, -11.2344,  ...,  -9.6328, -10.3516, -12.4375],
         [-13.5938, -13.5859, -13.9297,  ..., -11.6328, -10.5625, -10.7969],
         ...,
         [ -8.5703,  -8.8750,  -8.6328,  ...,  -7.0742,  -8.6797,  -6.3398],
         [ -3.4785,  -3.8359,  -3.3945,  ...,  -2.9336,  -3.1641,  -3.0977],
         [-15.5234, -15.4297, -15.4688,  ..., -12.4922, -14.3984, -13.1562]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 152, 30522])
tensor([[[-11.1406, -11.7344, -11.5625,  ..., -12.0859, -12.2344,  -9.1016],
         [-11.0703, -11.7188, -11.2344,  ...,  -9.6328, -10.3516, -12.4375],
         [-13.5938, -13.5859, -13.9297,  ..., -11.6328, -10.5625, -10.7969],
         ...,
         [ -8.5703,  -8.8750,  -8.6328,  ...,  -7.0742,  -8.6797,  -6.3398],
         [ -3.4785,  -3.8359,  -3.3945,  ...,  -2.9336,  -3.1641,  -3.0977],
         [-15.5234, -15.4297, -15.4688,  ..., -12.4922, -14.3984, -13.1562]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
torch.Size([152, 30522])
tensor([[-11.1406, -11.7344, -11.5625,  ..., -12.0859, -12.2344,  -9.1016],
        [-11.0703, -11.7188, -11.2344,  ...,  -9.6328, -10.3516, -12.4375],
        [-13.5938, -13.5859, -13.9297,  ..., -11.6328, -10.5625, -10.7969],
        ...,
        [ -8.5703,  -8.8750,  -8.6328,  ...,  -7.0742,  -8.6797,  -6.3398],
        [ -3.4785,  -3.8359,  -3.3945,  ...,  -2.9336,  -3.1641,  -3.0977],
        [-15.5234, -15.4297, -15.4688,  ..., -12.4922, -14.3984, -13.1562]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([152, 30522])
tensor([[-11.1406, -11.7344, -11.5625,  ..., -12.0859, -12.2344,  -9.1016],
        [-11.0703, -11.7188, -11.2344,  ...,  -9.6328, -10.3516, -12.4375],
        [-13.5938, -13.5859, -13.9297,  ..., -11.6328, -10.5625, -10.7969],
        ...,
        [ -8.5703,  -8.8750,  -8.6328,  ...,  -7.0742,  -8.6797,  -6.3398],
        [ -3.4785,  -3.8359,  -3.3945,  ...,  -2.9336,  -3.1641,  -3.0977],
        [-15.5234, -15.4297, -15.4688,  ..., -12.4922, -14.3984, -13.1562]],
       device='cuda:0', grad_fn=<SelectBackward0>)
Processing test sample 5/5 (after training)
MaskedLMOutput(loss=tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-13.5859, -14.4766, -14.0703,  ..., -12.5156, -13.5859,  -7.8828],
         [-10.7734, -11.3594, -11.1016,  ...,  -8.2422, -10.5469,  -8.9609],
         [-12.7422, -12.8125, -12.9375,  ..., -11.2500,  -9.4297,  -7.7617],
         ...,
         [-11.3594, -11.4531, -11.0859,  ...,  -8.3438, -11.3125,  -6.1953],
         [ -4.9102,  -5.2773,  -5.1328,  ...,  -4.9023,  -6.5859,  -4.7617],
         [-15.1406, -15.1562, -15.2344,  ..., -11.7500, -12.7891,  -9.5703]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>), hidden_states=None, attentions=None)
True
torch.Size([1, 21, 30522])
tensor([[[-13.5859, -14.4766, -14.0703,  ..., -12.5156, -13.5859,  -7.8828],
         [-10.7734, -11.3594, -11.1016,  ...,  -8.2422, -10.5469,  -8.9609],
         [-12.7422, -12.8125, -12.9375,  ..., -11.2500,  -9.4297,  -7.7617],
         ...,
         [-11.3594, -11.4531, -11.0859,  ...,  -8.3438, -11.3125,  -6.1953],
         [ -4.9102,  -5.2773,  -5.1328,  ...,  -4.9023,  -6.5859,  -4.7617],
         [-15.1406, -15.1562, -15.2344,  ..., -11.7500, -12.7891,  -9.5703]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
torch.Size([21, 30522])
tensor([[-13.5859, -14.4766, -14.0703,  ..., -12.5156, -13.5859,  -7.8828],
        [-10.7734, -11.3594, -11.1016,  ...,  -8.2422, -10.5469,  -8.9609],
        [-12.7422, -12.8125, -12.9375,  ..., -11.2500,  -9.4297,  -7.7617],
        ...,
        [-11.3594, -11.4531, -11.0859,  ...,  -8.3438, -11.3125,  -6.1953],
        [ -4.9102,  -5.2773,  -5.1328,  ...,  -4.9023,  -6.5859,  -4.7617],
        [-15.1406, -15.1562, -15.2344,  ..., -11.7500, -12.7891,  -9.5703]],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
torch.Size([21, 30522])
tensor([[-13.5859, -14.4766, -14.0703,  ..., -12.5156, -13.5859,  -7.8828],
        [-10.7734, -11.3594, -11.1016,  ...,  -8.2422, -10.5469,  -8.9609],
        [-12.7422, -12.8125, -12.9375,  ..., -11.2500,  -9.4297,  -7.7617],
        ...,
        [-11.3594, -11.4531, -11.0859,  ...,  -8.3438, -11.3125,  -6.1953],
        [ -4.9102,  -5.2773,  -5.1328,  ...,  -4.9023,  -6.5859,  -4.7617],
        [-15.1406, -15.1562, -15.2344,  ..., -11.7500, -12.7891,  -9.5703]],
       device='cuda:0', grad_fn=<SelectBackward0>)

Processing complete. Saved 5 results. Failed: 0
[main e90e93b] BERT Script Results for Run 59927 (Model: bert, Dataset: scienceqa, Epochs: 10, Commit: 1d57233)
 3 files changed, 418 insertions(+)
 create mode 100644 data/full/bert_results_59927_ds-scienceqa.pkl
 create mode 100644 slurm-outputs/slurm.59927.abakus11.out
To github.com:ngruenefeld/gradient-uncertainty.git
   1d57233..e90e93b  main -> main
