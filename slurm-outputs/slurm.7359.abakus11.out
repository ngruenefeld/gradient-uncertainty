slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: 5fe10bbe2de7fe11c57fbccf30fd6b6ddaaaa716
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Job number: 7359
Dataset: truthful
Model: llama-3.2-3b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 0
Mode: full
Streaming dataset: False
Quantization bits: None (full precision)
Full gradient: False
Response only: True
Normalize: False
Perturbation mode: rephrase
Number of perturbations: 10
Divergence: low
Skip evaluation: True
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [02:54<02:54, 174.55s/it]Downloading shards: 100%|██████████| 2/2 [03:51<00:00, 105.43s/it]Downloading shards: 100%|██████████| 2/2 [03:51<00:00, 115.80s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.27s/it]
Processing sample 1/817 (dataset index: 0)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 176.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 1 (dataset index 0): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 19.14 GiB is allocated by PyTorch, and 176.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 2/817 (dataset index: 1)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 85.06 MiB is free. Including non-PyTorch memory, this process has 19.46 GiB memory in use. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 90.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 2 (dataset index 1): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 85.06 MiB is free. Including non-PyTorch memory, this process has 19.46 GiB memory in use. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 90.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 3/817 (dataset index: 2)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 41.06 MiB is free. Including non-PyTorch memory, this process has 19.50 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 39.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 3 (dataset index 2): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 41.06 MiB is free. Including non-PyTorch memory, this process has 19.50 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 39.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 4/817 (dataset index: 3)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 95.06 MiB is free. Including non-PyTorch memory, this process has 19.45 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 50.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 4 (dataset index 3): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 95.06 MiB is free. Including non-PyTorch memory, this process has 19.45 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 50.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 5/817 (dataset index: 4)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 5 (dataset index 4): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 6/817 (dataset index: 5)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 23.06 MiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 49.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 6 (dataset index 5): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 23.06 MiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 49.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 7/817 (dataset index: 6)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 49.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 7 (dataset index 6): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 19.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 49.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 8/817 (dataset index: 7)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 71.06 MiB is free. Including non-PyTorch memory, this process has 19.47 GiB memory in use. Of the allocated memory 19.21 GiB is allocated by PyTorch, and 39.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 8 (dataset index 7): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 71.06 MiB is free. Including non-PyTorch memory, this process has 19.47 GiB memory in use. Of the allocated memory 19.21 GiB is allocated by PyTorch, and 39.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 9/817 (dataset index: 8)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 21.06 MiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 37.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 9 (dataset index 8): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 21.06 MiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 37.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 10/817 (dataset index: 9)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
slurmstepd-abakus11: error: *** JOB 7359 ON abakus11 CANCELLED AT 2025-05-30T19:02:30 ***
