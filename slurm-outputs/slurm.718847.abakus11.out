slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: b33dbf9ec8ed79a5018a39779ec84a9108ec5bcc
Job number: 718847
Dataset: trivia
Model: llama-3.2-3b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 5
Streaming dataset: True
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
Selected 5 random samples using reservoir sampling
Processing sample 1/5 (dataset index: 0)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 20.56 MiB is free. Including non-PyTorch memory, this process has 19.66 GiB memory in use. Of the allocated memory 19.36 GiB is allocated by PyTorch, and 79.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 1 (index 0): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 20.56 MiB is free. Including non-PyTorch memory, this process has 19.66 GiB memory in use. Of the allocated memory 19.36 GiB is allocated by PyTorch, and 79.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 2/5 (dataset index: 1)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 32.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 167.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 2 (index 1): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 32.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.26 GiB is allocated by PyTorch, and 167.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 3/5 (dataset index: 2)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 48.56 MiB is free. Including non-PyTorch memory, this process has 19.63 GiB memory in use. Of the allocated memory 19.37 GiB is allocated by PyTorch, and 37.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 3 (index 2): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 48.56 MiB is free. Including non-PyTorch memory, this process has 19.63 GiB memory in use. Of the allocated memory 19.37 GiB is allocated by PyTorch, and 37.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 4/5 (dataset index: 3)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 10.56 MiB is free. Including non-PyTorch memory, this process has 19.67 GiB memory in use. Of the allocated memory 19.40 GiB is allocated by PyTorch, and 44.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 4 (index 3): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 10.56 MiB is free. Including non-PyTorch memory, this process has 19.67 GiB memory in use. Of the allocated memory 19.40 GiB is allocated by PyTorch, and 44.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 5/5 (dataset index: 4)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 82.56 MiB is free. Including non-PyTorch memory, this process has 19.60 GiB memory in use. Of the allocated memory 19.33 GiB is allocated by PyTorch, and 45.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 5 (index 4): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 82.56 MiB is free. Including non-PyTorch memory, this process has 19.60 GiB memory in use. Of the allocated memory 19.33 GiB is allocated by PyTorch, and 45.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing complete, but no successful results to save. All 5 samples failed.
