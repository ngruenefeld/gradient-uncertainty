slurmstepd-aplit: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-aplit: error: Setting TMPDIR to /tmp
Running job with commit: 72e7de1de3774c00a29c47d1d8e0f7e1559264d8
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Job number: 7503
Dataset: truthful
Model: llama-3.1-8b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 0
Mode: full
Streaming dataset: False
Quantization bits: 4
Full gradient: False
Response only: True
Normalize: False
Perturbation mode: rephrase
Number of perturbations: 10
Divergence: medium
Skip evaluation: True
Loading model in 4-bit precision to reduce memory usage
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:15, 45.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:30<01:29, 44.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:14<00:44, 44.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:24<00:00, 31.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:24<00:00, 36.13s/it]
Processing sample 1/817 (dataset index: 0)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 711.31 MiB is free. Including non-PyTorch memory, this process has 6.73 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 266.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 1 (dataset index 0): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 711.31 MiB is free. Including non-PyTorch memory, this process has 6.73 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 266.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 2/817 (dataset index: 1)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 595.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 383.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 2 (dataset index 1): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 595.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 383.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 3/817 (dataset index: 2)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 381.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 3 (dataset index 2): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 381.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 4/817 (dataset index: 3)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 593.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 383.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 4 (dataset index 3): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 593.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 383.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 5/817 (dataset index: 4)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 379.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 5 (dataset index 4): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 379.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 6/817 (dataset index: 5)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 380.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 6 (dataset index 5): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 597.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 380.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 7/817 (dataset index: 6)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 603.31 MiB is free. Including non-PyTorch memory, this process has 6.83 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 374.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 7 (dataset index 6): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 603.31 MiB is free. Including non-PyTorch memory, this process has 6.83 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 374.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 8/817 (dataset index: 7)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 681.31 MiB is free. Including non-PyTorch memory, this process has 6.76 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 292.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 8 (dataset index 7): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 681.31 MiB is free. Including non-PyTorch memory, this process has 6.76 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 292.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 9/817 (dataset index: 8)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 599.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 378.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 9 (dataset index 8): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 599.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 378.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 10/817 (dataset index: 9)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 599.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 378.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 10 (dataset index 9): CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 7.57 GiB of which 599.31 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 378.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 11/817 (dataset index: 10)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
slurmstepd-aplit: error: *** JOB 7503 ON aplit CANCELLED AT 2025-05-31T14:27:49 ***
