slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: 4e677ed0822446a82286e85058aa0af66e94f284
Job number: 718842
Dataset: trivia
Model: llama-3.2-3b
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]
Processing sample 1/5 (dataset index: 262)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 8.56 MiB is free. Including non-PyTorch memory, this process has 19.67 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 203.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 1 (index 262): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 8.56 MiB is free. Including non-PyTorch memory, this process has 19.67 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 203.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 2/5 (dataset index: 5570)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 70.56 MiB is free. Including non-PyTorch memory, this process has 19.61 GiB memory in use. Of the allocated memory 19.33 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 2 (index 5570): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 70.56 MiB is free. Including non-PyTorch memory, this process has 19.61 GiB memory in use. Of the allocated memory 19.33 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 3/5 (dataset index: 396)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 26.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 273.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 3 (index 396): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 26.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 273.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 4/5 (dataset index: 4115)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 24.56 MiB is free. Including non-PyTorch memory, this process has 19.66 GiB memory in use. Of the allocated memory 19.37 GiB is allocated by PyTorch, and 64.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 4 (index 4115): CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 24.56 MiB is free. Including non-PyTorch memory, this process has 19.66 GiB memory in use. Of the allocated memory 19.37 GiB is allocated by PyTorch, and 64.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 5/5 (dataset index: 6403)
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Error in completion_gradient: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 32.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 179.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 5 (index 6403): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.71 GiB of which 32.56 MiB is free. Including non-PyTorch memory, this process has 19.65 GiB memory in use. Of the allocated memory 19.25 GiB is allocated by PyTorch, and 179.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing complete, but no successful results to save. All 5 samples failed.
