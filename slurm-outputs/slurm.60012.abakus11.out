slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: 522a43405fe052bfd312987e70ed8e3614e40ddd
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package wordnet to
[nltk_data]     /home/g/gruenefeld/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Job number: 60012
Dataset: finefineweb
Model: medicine-chat
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Mode: test
Quantization bits: 4
Full gradient: False
Response only: True
Normalize: False
Perturbation mode: rephrase
Number of perturbations: 3
Max tokens: 0
Sample Size Per Label: 5
Loading model in 4-bit precision to reduce memory usage
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.18s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.94s/it]
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Processing sample 1/50 (dataset index: 0)
Processing sample 1 (dataset index 0)
torch.Size([1, 690, 32001])
torch.Size([690])
Error processing sample 1 (dataset index 0): too many values to unpack (expected 2)
Processing sample 2/50 (dataset index: 1)
Processing sample 2 (dataset index 1)
torch.Size([1, 493, 32001])
torch.Size([493])
Error processing sample 2 (dataset index 1): too many values to unpack (expected 2)
Processing sample 3/50 (dataset index: 2)
Processing sample 3 (dataset index 2)
torch.Size([1, 2331, 32001])
torch.Size([2331])
Error processing sample 3 (dataset index 2): too many values to unpack (expected 2)
Processing sample 4/50 (dataset index: 3)
Processing sample 4 (dataset index 3)
torch.Size([1, 1952, 32001])
torch.Size([1952])
Error processing sample 4 (dataset index 3): too many values to unpack (expected 2)
Processing sample 5/50 (dataset index: 4)
Processing sample 5 (dataset index 4)
torch.Size([1, 878, 32001])
torch.Size([878])
Error processing sample 5 (dataset index 4): too many values to unpack (expected 2)
Processing sample 6/50 (dataset index: 5)
Processing sample 6 (dataset index 5)
torch.Size([1, 1453, 32001])
torch.Size([1453])
Error processing sample 6 (dataset index 5): too many values to unpack (expected 2)
Processing sample 7/50 (dataset index: 6)
Processing sample 7 (dataset index 6)
torch.Size([1, 1310, 32001])
torch.Size([1310])
Error processing sample 7 (dataset index 6): too many values to unpack (expected 2)
Processing sample 8/50 (dataset index: 7)
Processing sample 8 (dataset index 7)
torch.Size([1, 704, 32001])
torch.Size([704])
Error processing sample 8 (dataset index 7): too many values to unpack (expected 2)
Processing sample 9/50 (dataset index: 8)
Processing sample 9 (dataset index 8)
Error in completion_gradient: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 18.47 GiB is allocated by PyTorch, and 864.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 9 (dataset index 8): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 10.06 MiB is free. Including non-PyTorch memory, this process has 19.54 GiB memory in use. Of the allocated memory 18.47 GiB is allocated by PyTorch, and 864.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 10/50 (dataset index: 9)
Processing sample 10 (dataset index 9)
torch.Size([1, 793, 32001])
torch.Size([793])
Error processing sample 10 (dataset index 9): too many values to unpack (expected 2)
Processing sample 11/50 (dataset index: 10)
Processing sample 11 (dataset index 10)
torch.Size([1, 829, 32001])
torch.Size([829])
Error processing sample 11 (dataset index 10): too many values to unpack (expected 2)
Processing sample 12/50 (dataset index: 11)
Processing sample 12 (dataset index 11)
torch.Size([1, 1588, 32001])
torch.Size([1588])
Error processing sample 12 (dataset index 11): too many values to unpack (expected 2)
Processing sample 13/50 (dataset index: 12)
Processing sample 13 (dataset index 12)
torch.Size([1, 899, 32001])
torch.Size([899])
Error processing sample 13 (dataset index 12): too many values to unpack (expected 2)
Processing sample 14/50 (dataset index: 13)
Processing sample 14 (dataset index 13)
torch.Size([1, 250, 32001])
torch.Size([250])
Error processing sample 14 (dataset index 13): too many values to unpack (expected 2)
Processing sample 15/50 (dataset index: 14)
Processing sample 15 (dataset index 14)
torch.Size([1, 421, 32001])
torch.Size([421])
Error processing sample 15 (dataset index 14): too many values to unpack (expected 2)
Processing sample 16/50 (dataset index: 15)
Processing sample 16 (dataset index 15)
torch.Size([1, 659, 32001])
torch.Size([659])
Error processing sample 16 (dataset index 15): too many values to unpack (expected 2)
Processing sample 17/50 (dataset index: 16)
Processing sample 17 (dataset index 16)
torch.Size([1, 352, 32001])
torch.Size([352])
Error processing sample 17 (dataset index 16): too many values to unpack (expected 2)
Processing sample 18/50 (dataset index: 17)
Processing sample 18 (dataset index 17)
torch.Size([1, 722, 32001])
torch.Size([722])
Error processing sample 18 (dataset index 17): too many values to unpack (expected 2)
Processing sample 19/50 (dataset index: 18)
Processing sample 19 (dataset index 18)
torch.Size([1, 1555, 32001])
torch.Size([1555])
Error processing sample 19 (dataset index 18): too many values to unpack (expected 2)
Processing sample 20/50 (dataset index: 19)
Processing sample 20 (dataset index 19)
torch.Size([1, 217, 32001])
torch.Size([217])
Error processing sample 20 (dataset index 19): too many values to unpack (expected 2)
Processing sample 21/50 (dataset index: 20)
Processing sample 21 (dataset index 20)
torch.Size([1, 284, 32001])
torch.Size([284])
Error processing sample 21 (dataset index 20): too many values to unpack (expected 2)
Processing sample 22/50 (dataset index: 21)
Processing sample 22 (dataset index 21)
torch.Size([1, 175, 32001])
torch.Size([175])
Error processing sample 22 (dataset index 21): too many values to unpack (expected 2)
Processing sample 23/50 (dataset index: 22)
Processing sample 23 (dataset index 22)
torch.Size([1, 258, 32001])
torch.Size([258])
Error processing sample 23 (dataset index 22): too many values to unpack (expected 2)
Processing sample 24/50 (dataset index: 23)
Processing sample 24 (dataset index 23)
torch.Size([1, 658, 32001])
torch.Size([658])
Error processing sample 24 (dataset index 23): too many values to unpack (expected 2)
Processing sample 25/50 (dataset index: 24)
Processing sample 25 (dataset index 24)
torch.Size([1, 207, 32001])
torch.Size([207])
Error processing sample 25 (dataset index 24): too many values to unpack (expected 2)
Processing sample 26/50 (dataset index: 25)
Processing sample 26 (dataset index 25)
torch.Size([1, 549, 32001])
torch.Size([549])
Error processing sample 26 (dataset index 25): too many values to unpack (expected 2)
Processing sample 27/50 (dataset index: 26)
Processing sample 27 (dataset index 26)
torch.Size([1, 864, 32001])
torch.Size([864])
Error processing sample 27 (dataset index 26): too many values to unpack (expected 2)
Processing sample 28/50 (dataset index: 27)
Processing sample 28 (dataset index 27)
torch.Size([1, 449, 32001])
torch.Size([449])
Error processing sample 28 (dataset index 27): too many values to unpack (expected 2)
Processing sample 29/50 (dataset index: 28)
Processing sample 29 (dataset index 28)
torch.Size([1, 368, 32001])
torch.Size([368])
Error processing sample 29 (dataset index 28): too many values to unpack (expected 2)
Processing sample 30/50 (dataset index: 29)
Processing sample 30 (dataset index 29)
torch.Size([1, 2423, 32001])
torch.Size([2423])
Error processing sample 30 (dataset index 29): too many values to unpack (expected 2)
Processing sample 31/50 (dataset index: 30)
Processing sample 31 (dataset index 30)
torch.Size([1, 1721, 32001])
torch.Size([1721])
Error processing sample 31 (dataset index 30): too many values to unpack (expected 2)
Processing sample 32/50 (dataset index: 31)
Processing sample 32 (dataset index 31)
torch.Size([1, 477, 32001])
torch.Size([477])
Error processing sample 32 (dataset index 31): too many values to unpack (expected 2)
Processing sample 33/50 (dataset index: 32)
Processing sample 33 (dataset index 32)
torch.Size([1, 336, 32001])
torch.Size([336])
Error processing sample 33 (dataset index 32): too many values to unpack (expected 2)
Processing sample 34/50 (dataset index: 33)
Processing sample 34 (dataset index 33)
Error in completion_gradient: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 19.49 GiB memory in use. Of the allocated memory 18.40 GiB is allocated by PyTorch, and 886.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 34 (dataset index 33): CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 60.06 MiB is free. Including non-PyTorch memory, this process has 19.49 GiB memory in use. Of the allocated memory 18.40 GiB is allocated by PyTorch, and 886.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 35/50 (dataset index: 34)
Processing sample 35 (dataset index 34)
torch.Size([1, 800, 32001])
torch.Size([800])
Error processing sample 35 (dataset index 34): too many values to unpack (expected 2)
Processing sample 36/50 (dataset index: 35)
Processing sample 36 (dataset index 35)
torch.Size([1, 280, 32001])
torch.Size([280])
Error processing sample 36 (dataset index 35): too many values to unpack (expected 2)
Processing sample 37/50 (dataset index: 36)
Processing sample 37 (dataset index 36)
Error in completion_gradient: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 19.55 GiB memory in use. Of the allocated memory 17.83 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 37 (dataset index 36): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 4.06 MiB is free. Including non-PyTorch memory, this process has 19.55 GiB memory in use. Of the allocated memory 17.83 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 38/50 (dataset index: 37)
Processing sample 38 (dataset index 37)
torch.Size([1, 440, 32001])
torch.Size([440])
Error processing sample 38 (dataset index 37): too many values to unpack (expected 2)
Processing sample 39/50 (dataset index: 38)
Processing sample 39 (dataset index 38)
torch.Size([1, 1299, 32001])
torch.Size([1299])
Error processing sample 39 (dataset index 38): too many values to unpack (expected 2)
Processing sample 40/50 (dataset index: 39)
Processing sample 40 (dataset index 39)
torch.Size([1, 912, 32001])
torch.Size([912])
Error processing sample 40 (dataset index 39): too many values to unpack (expected 2)
Processing sample 41/50 (dataset index: 40)
Processing sample 41 (dataset index 40)
torch.Size([1, 352, 32001])
torch.Size([352])
Error processing sample 41 (dataset index 40): too many values to unpack (expected 2)
Processing sample 42/50 (dataset index: 41)
Processing sample 42 (dataset index 41)
torch.Size([1, 205, 32001])
torch.Size([205])
Error processing sample 42 (dataset index 41): too many values to unpack (expected 2)
Processing sample 43/50 (dataset index: 42)
Processing sample 43 (dataset index 42)
torch.Size([1, 2479, 32001])
torch.Size([2479])
Error processing sample 43 (dataset index 42): too many values to unpack (expected 2)
Processing sample 44/50 (dataset index: 43)
Processing sample 44 (dataset index 43)
Error in completion_gradient: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 18.10 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error calculating gradient for sample 44 (dataset index 43): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 22.06 MiB is free. Including non-PyTorch memory, this process has 19.53 GiB memory in use. Of the allocated memory 18.10 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing sample 45/50 (dataset index: 44)
Processing sample 45 (dataset index 44)
torch.Size([1, 222, 32001])
torch.Size([222])
Error processing sample 45 (dataset index 44): too many values to unpack (expected 2)
Processing sample 46/50 (dataset index: 45)
Processing sample 46 (dataset index 45)
torch.Size([1, 187, 32001])
torch.Size([187])
Error processing sample 46 (dataset index 45): too many values to unpack (expected 2)
Processing sample 47/50 (dataset index: 46)
Processing sample 47 (dataset index 46)
torch.Size([1, 199, 32001])
torch.Size([199])
Error processing sample 47 (dataset index 46): too many values to unpack (expected 2)
Processing sample 48/50 (dataset index: 47)
Processing sample 48 (dataset index 47)
torch.Size([1, 1180, 32001])
torch.Size([1180])
Error processing sample 48 (dataset index 47): too many values to unpack (expected 2)
Processing sample 49/50 (dataset index: 48)
Processing sample 49 (dataset index 48)
torch.Size([1, 988, 32001])
torch.Size([988])
Error processing sample 49 (dataset index 48): too many values to unpack (expected 2)
Processing sample 50/50 (dataset index: 49)
Processing sample 50 (dataset index 49)
torch.Size([1, 778, 32001])
torch.Size([778])
Error processing sample 50 (dataset index 49): too many values to unpack (expected 2)
Processing complete, but no successful results to save. All 50 samples failed.
[main 1669a8c] Domain Specific LLM Script Results for Run 60012 (Commit: 522a434)
 2 files changed, 284 insertions(+)
 create mode 100644 slurm-outputs/slurm.60012.abakus11.out
To github.com:ngruenefeld/gradient-uncertainty.git
   522a434..1669a8c  main -> main
