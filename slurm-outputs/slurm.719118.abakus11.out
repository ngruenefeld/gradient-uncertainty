slurmstepd-abakus11: error: Unable to create TMPDIR [/tmp/user/24470]: Permission denied
slurmstepd-abakus11: error: Setting TMPDIR to /tmp
Running job with commit: fa306eba66df30807fc622801cc7ee6d3162ac86
Job number: 719118
Dataset: truthful
Model: deepseek-v3
GPT Model: gpt-4o-mini-2024-07-18
Key mode: keyfile
Sample size: 0
Streaming dataset: False
Quantization bits: 4
Loading model in 4-bit precision to reduce memory usage
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-v3:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-v3:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/qa.py", line 430, in <module>
    main(args)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/scripts/qa.py", line 127, in main
    model = AutoModelForCausalLM.from_pretrained(model_path, **model_load_params)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 559, in from_pretrained
    return model_class.from_pretrained(
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3647, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.8/site-packages/transformers/quantizers/auto.py", line 173, in merge_quantization_configs
    quantization_config = AutoQuantizationConfig.from_dict(quantization_config)
  File "/home/g/gruenefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.8/site-packages/transformers/quantizers/auto.py", line 97, in from_dict
    raise ValueError(
ValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet']
[main c4ee473] QA Script Results for Run 719118 (Commit: fa306eb)
 2 files changed, 41 insertions(+)
 create mode 100644 slurm-outputs/slurm.719118.abakus11.out
To github.com:ngruenefeld/gradient-uncertainty.git
   fa306eb..c4ee473  main -> main
