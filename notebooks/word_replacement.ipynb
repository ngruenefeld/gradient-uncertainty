{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from transformers import AutoTokenizer\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(word, lang=\"en\"):\n",
    "    supported_languages = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ko\", \"pt\", \"ru\", \"zh\"]\n",
    "    print(lang)\n",
    "    print(lang in supported_languages)\n",
    "    \n",
    "    if lang not in supported_languages:\n",
    "        raise ValueError(f\"Unsupported language. Supported languages: {supported_languages}\")\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word and \"_\" not in lemma.name():\n",
    "                    synonyms.append(lemma.name())\n",
    "        \n",
    "        if not synonyms:\n",
    "            return word\n",
    "        return random.choice(synonyms)\n",
    "    \n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas(lang=lang):\n",
    "            if lemma.name() != word and \"_\" not in lemma.name():\n",
    "                synonyms.append(lemma.name())\n",
    "    \n",
    "    # If no synonyms found in the target language, optionally try English as fallback\n",
    "    if not synonyms:\n",
    "        # Uncomment below to fall back to English synonyms when none found in target language\n",
    "        # return get_synonym(word, \"en\")\n",
    "        return word\n",
    "    \n",
    "    return random.choice(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Languages supported\n",
    "LANGUAGES = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ko\", \"pt\", \"ru\", \"zh\"]\n",
    "\n",
    "def get_synonym(word, lang=\"en\", tokenizer=None):\n",
    "    if lang not in LANGUAGES:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    try:\n",
    "        # Translate to English if not already\n",
    "        word_en = word if lang == \"en\" else GoogleTranslator(source=lang, target='en').translate(word)\n",
    "\n",
    "        # Get English synonyms from WordNet\n",
    "        synsets = wordnet.synsets(word_en)\n",
    "        synonym_candidates = set()\n",
    "        for syn in synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ')\n",
    "                if synonym.lower() != word_en.lower():\n",
    "                    synonym_candidates.add(synonym)\n",
    "\n",
    "        if not synonym_candidates:\n",
    "            return None\n",
    "\n",
    "        if tokenizer:\n",
    "            synonyms = []\n",
    "            for synonym in list(synonym_candidates):\n",
    "                if lang == \"en\":\n",
    "                    if len(tokenizer.tokenize(synonym)) == 1:\n",
    "                        synonyms.append(synonym)\n",
    "                else:\n",
    "                    translated_syn = GoogleTranslator(source='en', target=lang).translate(synonym)\n",
    "                    if len(tokenizer.tokenize(translated_syn)) == 1:\n",
    "                        synonyms.append(synonym)\n",
    "        else:\n",
    "            synonyms = synonym_candidates\n",
    "        \n",
    "        if len(synonyms) == 0:\n",
    "            return None\n",
    "\n",
    "        # Choose a random synonym\n",
    "        chosen_syn = random.choice(list(synonyms))\n",
    "\n",
    "        # Translate back to original language if needed\n",
    "        return chosen_syn if lang == \"en\" else GoogleTranslator(source='en', target=lang).translate(chosen_syn)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_word(token, tokenizer):\n",
    "    return tokenizer.decode([token]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tokens_with_synonyms(inputs, tokenizer, device, lang=\"en\", replacement_prob=0.15):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].clone()\n",
    "\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if random.random() < replacement_prob:\n",
    "                token_id = input_ids[i, j].item()\n",
    "                word = token_to_word(token_id, tokenizer)\n",
    "\n",
    "                if (\n",
    "                    word.lower() in stop_words\n",
    "                    or word.startswith(\"##\")\n",
    "                    or not word.isalpha()\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                synonym = get_synonym(word, lang=lang, tokenizer=tokenizer)\n",
    "                if not synonym:\n",
    "                    synonym = word\n",
    "\n",
    "                synonym_tokens = tokenizer(\n",
    "                    synonym, return_tensors=\"pt\", add_special_tokens=False\n",
    "                ).to(device)\n",
    "\n",
    "                if synonym_tokens[\"input_ids\"].shape[1] == 1:\n",
    "                    if synonym_tokens[\"input_ids\"][0, 0] != token_id:\n",
    "                        input_ids[i, j] = synonym_tokens[\"input_ids\"][0, 0]\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Modified: the quick brownish confuse jumps over the lazy tail.\n"
     ]
    }
   ],
   "source": [
    "modified_input_ids = replace_tokens_with_synonyms(inputs, tokenizer, device, replacement_prob=0.5)\n",
    "modified_sentence = tokenizer.decode(modified_input_ids[0])\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Modified: {modified_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_it = \"Il video mostra un gruppo di ballerini che esegue una coreografia di danza Jazz in un ambiente chiuso, probabilmente uno studio di danza.\"\n",
    "sample_zh = \"该视频展示了一群舞者在一个封闭的环境中执行爵士舞编舞，可能是一个舞蹈工作室。\"\n",
    "sample_de = \"Das Video zeigt eine Gruppe von Tänzern, die in einer geschlossenen Umgebung, wahrscheinlich einem Tanzstudio, eine Jazz-Choreografie ausführen.\"\n",
    "sample_fr = \"La vidéo montre un groupe de danseurs exécutant une chorégraphie de danse jazz dans un environnement clos, probablement un studio de danse.\"\n",
    "sample_es = \"El video muestra a un grupo de bailarines realizando una coreografía de danza jazz en un entorno cerrado, probablemente un estudio de danza.\"\n",
    "sample_pt = \"O vídeo mostra um grupo de dançarinos executando uma coreografia de dança jazz em um ambiente fechado, provavelmente um estúdio de dança.\"\n",
    "sample_ru = \"В видео показана группа танцоров, исполняющих джазовую хореографию в закрытом помещении, вероятно, в танцевальной студии.\"\n",
    "sample_ko = \"이 비디오는 아마도 댄스 스튜디오에서 닫힌 환경에서 재즈 댄스 안무를 수행하는 무용수 그룹을 보여줍니다.\"\n",
    "\n",
    "lang_samples = {\n",
    "    \"it\": sample_it,\n",
    "    \"zh\": sample_zh,\n",
    "    \"de\": sample_de,\n",
    "    \"fr\": sample_fr,\n",
    "    \"es\": sample_es,\n",
    "    \"pt\": sample_pt,\n",
    "    \"ru\": sample_ru,\n",
    "    \"ko\": sample_ko\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing ᄃ with 100\n",
      "Replacing ᄃ with 100\n",
      "Original: 이 비디오는 아마도 댄스 스튜디오에서 닫힌 환경에서 재즈 댄스 안무를 수행하는 무용수 그룹을 보여줍니다.\n",
      "Modified: 이 비디오는 아마도 100ᅢᆫ스 스튜디오에서 [UNK] 환경에서 재즈 100ᅢᆫ스 안무를 수행하는 무용수 그룹을 보여줍니다.\n"
     ]
    }
   ],
   "source": [
    "lang_choice = \"ko\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    lang_samples[lang_choice],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(device)\n",
    "\n",
    "modified_input_ids = replace_tokens_with_synonyms(inputs, tokenizer, device, lang=lang_choice, replacement_prob=1)\n",
    "modified_sentence = tokenizer.decode(modified_input_ids[0])\n",
    "print(f\"Original: {lang_samples[lang_choice]}\")\n",
    "print(f\"Modified: {modified_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('omw-1.4')  # Required for multilingual WordNet\n",
    "\n",
    "def get_synonyms_in_language(word, lang='es'):\n",
    "    synsets = wn.synsets(word, lang=lang)\n",
    "    synonyms = set()\n",
    "    print(synonyms)\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas(lang):\n",
    "            lemma_name = lemma.name().replace('_', ' ')\n",
    "            if lemma_name.lower() != word.lower():\n",
    "                synonyms.add(lemma_name)\n",
    "    return list(synonyms) if synonyms else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eng',\n",
       " 'als',\n",
       " 'arb',\n",
       " 'bul',\n",
       " 'cmn',\n",
       " 'dan',\n",
       " 'ell',\n",
       " 'fin',\n",
       " 'fra',\n",
       " 'heb',\n",
       " 'hrv',\n",
       " 'isl',\n",
       " 'ita',\n",
       " 'ita_iwn',\n",
       " 'jpn',\n",
       " 'cat',\n",
       " 'eus',\n",
       " 'glg',\n",
       " 'spa',\n",
       " 'ind',\n",
       " 'zsm',\n",
       " 'nld',\n",
       " 'nno',\n",
       " 'nob',\n",
       " 'pol',\n",
       " 'por',\n",
       " 'ron',\n",
       " 'lit',\n",
       " 'slk',\n",
       " 'slv',\n",
       " 'swe',\n",
       " 'tha']"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['les boules',\n",
       " 'baiser',\n",
       " 'foutre',\n",
       " 'vis',\n",
       " 'étron',\n",
       " 'foirer',\n",
       " 'bousiller',\n",
       " 'cric',\n",
       " 'niquer',\n",
       " 'faire',\n",
       " 'enculer',\n",
       " 'putain',\n",
       " 'rendre',\n",
       " 'valet',\n",
       " 'caguer',\n",
       " 'taureau',\n",
       " 'baise',\n",
       " 'gâcher',\n",
       " 'baisage',\n",
       " 'chier',\n",
       " 'repriser',\n",
       " 'déféquer']"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms_in_language(\"merde\", lang=\"fra\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
