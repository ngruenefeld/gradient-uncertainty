{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(word, lang=\"en\"):\n",
    "    supported_languages = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ko\", \"pt\", \"ru\", \"zh\"]\n",
    "    print(lang)\n",
    "    print(lang in supported_languages)\n",
    "    \n",
    "    if lang not in supported_languages:\n",
    "        raise ValueError(f\"Unsupported language. Supported languages: {supported_languages}\")\n",
    "    \n",
    "    if lang == \"en\":\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word and \"_\" not in lemma.name():\n",
    "                    synonyms.append(lemma.name())\n",
    "        \n",
    "        if not synonyms:\n",
    "            return word\n",
    "        return random.choice(synonyms)\n",
    "    \n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas(lang=lang):\n",
    "            if lemma.name() != word and \"_\" not in lemma.name():\n",
    "                synonyms.append(lemma.name())\n",
    "    \n",
    "    # If no synonyms found in the target language, optionally try English as fallback\n",
    "    if not synonyms:\n",
    "        # Uncomment below to fall back to English synonyms when none found in target language\n",
    "        # return get_synonym(word, \"en\")\n",
    "        return word\n",
    "    \n",
    "    return random.choice(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_LANGUAGES = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ko\", \"pt\", \"ru\", \"zh\"]\n",
    "\n",
    "def get_synonym(word, lang='en'):\n",
    "    if lang not in SUPPORTED_LANGUAGES:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    synonyms = set()\n",
    "\n",
    "    # Retrieve English synsets (WordNet is English-based)\n",
    "    synsets = wordnet.synsets(word, lang='eng') if lang != 'en' else wordnet.synsets(word)\n",
    "\n",
    "    for syn in synsets:\n",
    "        if lang == 'en':\n",
    "            for lemma in syn.lemmas():\n",
    "                name = lemma.name()\n",
    "                if name != word and \"_\" not in name:\n",
    "                    synonyms.add(name)\n",
    "        else:\n",
    "            for lemma in syn.lemmas(lang=lang):\n",
    "                name = lemma.name()\n",
    "                if name != word and \"_\" not in name:\n",
    "                    synonyms.add(name)\n",
    "\n",
    "    return random.choice(list(synonyms)) if synonyms else word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_word(token, tokenizer):\n",
    "    return tokenizer.decode([token]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tokens_with_synonyms(inputs, tokenizer, device, replacement_prob=0.15):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].clone()\n",
    "\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if random.random() < replacement_prob:\n",
    "                token_id = input_ids[i, j].item()\n",
    "                word = token_to_word(token_id, tokenizer)\n",
    "\n",
    "                if (\n",
    "                    word.lower() in stop_words\n",
    "                    or word.startswith(\"##\")\n",
    "                    or not word.isalpha()\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                synonym = get_synonym(word)\n",
    "\n",
    "                synonym_tokens = tokenizer(\n",
    "                    synonym, return_tensors=\"pt\", add_special_tokens=False\n",
    "                ).to(device)\n",
    "\n",
    "                if synonym_tokens[\"input_ids\"].shape[1] == 1:\n",
    "                    input_ids[i, j] = synonym_tokens[\"input_ids\"][0, 0]\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Modified: the quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "modified_input_ids = replace_tokens_with_synonyms(inputs, tokenizer, device, replacement_prob=0.5)\n",
    "modified_sentence = tokenizer.decode(modified_input_ids[0])\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Modified: {modified_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "Synset('dog.n.01')\n",
      "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n",
      "Synset('frump.n.01')\n",
      "[Lemma('frump.n.01.frump'), Lemma('frump.n.01.dog')]\n",
      "Synset('dog.n.03')\n",
      "[Lemma('dog.n.03.dog')]\n",
      "Synset('cad.n.01')\n",
      "[Lemma('cad.n.01.cad'), Lemma('cad.n.01.bounder'), Lemma('cad.n.01.blackguard'), Lemma('cad.n.01.dog'), Lemma('cad.n.01.hound'), Lemma('cad.n.01.heel')]\n",
      "Synset('frank.n.02')\n",
      "[Lemma('frank.n.02.frank'), Lemma('frank.n.02.frankfurter'), Lemma('frank.n.02.hotdog'), Lemma('frank.n.02.hot_dog'), Lemma('frank.n.02.dog'), Lemma('frank.n.02.wiener'), Lemma('frank.n.02.wienerwurst'), Lemma('frank.n.02.weenie')]\n",
      "Synset('pawl.n.01')\n",
      "[Lemma('pawl.n.01.pawl'), Lemma('pawl.n.01.detent'), Lemma('pawl.n.01.click'), Lemma('pawl.n.01.dog')]\n",
      "Synset('andiron.n.01')\n",
      "[Lemma('andiron.n.01.andiron'), Lemma('andiron.n.01.firedog'), Lemma('andiron.n.01.dog'), Lemma('andiron.n.01.dog-iron')]\n",
      "Synset('chase.v.01')\n",
      "[Lemma('chase.v.01.chase'), Lemma('chase.v.01.chase_after'), Lemma('chase.v.01.trail'), Lemma('chase.v.01.tail'), Lemma('chase.v.01.tag'), Lemma('chase.v.01.give_chase'), Lemma('chase.v.01.dog'), Lemma('chase.v.01.go_after'), Lemma('chase.v.01.track')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cad'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonym(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국어'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonym(\"한국어\", lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "Language is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWordNetError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[338]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(f\"English synonym for 'good': {get_synonym('good', 'en')}\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGerman synonym for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgut\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mget_synonym\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgut\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mde\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpanish synonym for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbueno\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_synonym(\u001b[33m'\u001b[39m\u001b[33mbueno\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFrench synonym for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbon\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_synonym(\u001b[33m'\u001b[39m\u001b[33mbon\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfr\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[337]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_synonym\u001b[39m\u001b[34m(word, lang)\u001b[39m\n\u001b[32m     17\u001b[39m             synonyms.add(name)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m \u001b[43msyn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemmas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     20\u001b[39m         name = lemma.name()\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m name != word \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/gradient-uncertainty/env/lib/python3.13/site-packages/nltk/corpus/reader/wordnet.py:502\u001b[39m, in \u001b[36mSynset.lemmas\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lemmas\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wordnet_corpus_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_lang_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m     lemmark = []\n\u001b[32m    504\u001b[39m     lemmy = \u001b[38;5;28mself\u001b[39m.lemma_names(lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/gradient-uncertainty/env/lib/python3.13/site-packages/nltk/corpus/reader/wordnet.py:1323\u001b[39m, in \u001b[36mWordNetCorpusReader._load_lang_data\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_omw()\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langs():\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WordNetError(\u001b[33m\"\u001b[39m\u001b[33mLanguage is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exomw_reader \u001b[38;5;129;01mand\u001b[39;00m lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.omw_langs:\n\u001b[32m   1326\u001b[39m     reader = \u001b[38;5;28mself\u001b[39m._exomw_reader\n",
      "\u001b[31mWordNetError\u001b[39m: Language is not supported."
     ]
    }
   ],
   "source": [
    "# print(f\"English synonym for 'good': {get_synonym('good', 'en')}\")\n",
    "print(f\"German synonym for 'gut': {get_synonym('gut', 'de')}\")\n",
    "print(f\"Spanish synonym for 'bueno': {get_synonym('bueno', 'es')}\")\n",
    "print(f\"French synonym for 'bon': {get_synonym('bon', 'fr')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
