{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nilsgrunefeld/Documents/GitHub/gradient-uncertainty/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from transformers import AutoTokenizer\n",
    "from deep_translator import GoogleTranslator\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nilsgrunefeld/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISO_639_1_TO_3 = {\n",
    "    \"en\": \"eng\",\n",
    "    \"de\": None,\n",
    "    \"es\": \"spa\",\n",
    "    \"fr\": \"fra\",\n",
    "    \"it\": \"ita\",\n",
    "    \"ko\": None,\n",
    "    \"pt\": \"por\",\n",
    "    \"ru\": None,\n",
    "    \"zh\": \"cmn\",\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_translate(text, source, target):\n",
    "    if source == \"zh\":\n",
    "        source = \"zh-CN\"\n",
    "    if target == \"zh\":\n",
    "        target = \"zh-CN\"\n",
    "    return GoogleTranslator(source=source, target=target).translate(text)\n",
    "\n",
    "\n",
    "def get_german_synonyms(word):\n",
    "    try:\n",
    "        url = \"https://www.openthesaurus.de/synonyme/search\"\n",
    "        params = {\"q\": word, \"format\": \"application/json\"}\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        synonyms = set()\n",
    "        for synset in data.get(\"synsets\", []):\n",
    "            for term in synset.get(\"terms\", []):\n",
    "                if term[\"term\"].lower() != word.lower():\n",
    "                    synonyms.add(term[\"term\"])\n",
    "        return list(synonyms) if synonyms else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching German synonyms: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_omw_synonyms(word, lang):\n",
    "    omw_lang = ISO_639_1_TO_3.get(lang)\n",
    "    if not omw_lang:\n",
    "        return None\n",
    "    try:\n",
    "        synsets = wordnet.synsets(word, lang=omw_lang)\n",
    "        synonyms = set()\n",
    "        for syn in synsets:\n",
    "            for lemma in syn.lemmas(lang=omw_lang):\n",
    "                synonym = lemma.name().replace(\"_\", \" \")\n",
    "                if synonym.lower() != word.lower():\n",
    "                    synonyms.add(synonym)\n",
    "        return list(synonyms) if synonyms else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_synonym(word, lang=\"en\", tokenizer=None):\n",
    "    supported_languages = [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ko\", \"pt\", \"ru\", \"zh\"]\n",
    "\n",
    "    if lang not in supported_languages:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Try native synonym lookup\n",
    "        if lang == \"de\":\n",
    "            synonyms = get_german_synonyms(word)\n",
    "        else:\n",
    "            synonyms = get_omw_synonyms(word, lang)\n",
    "\n",
    "        # Step 2: If no native synonyms, fall back to English-based method\n",
    "        if not synonyms:\n",
    "            word_en = word if lang == \"en\" else cached_translate(word, lang, \"en\")\n",
    "            synsets = wordnet.synsets(word_en)\n",
    "            synonym_candidates = set()\n",
    "            for syn in synsets:\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonym = lemma.name().replace(\"_\", \" \")\n",
    "                    if synonym.lower() != word_en.lower():\n",
    "                        synonym_candidates.add(synonym)\n",
    "\n",
    "            if not synonym_candidates:\n",
    "                return None\n",
    "\n",
    "            if tokenizer:\n",
    "                valid_synonyms = []\n",
    "                random.shuffle(list(synonym_candidates))\n",
    "                for syn in synonym_candidates:\n",
    "                    if lang == \"en\":\n",
    "                        if len(tokenizer.tokenize(syn)) == 1:\n",
    "                            valid_synonyms.append(syn)\n",
    "                    else:\n",
    "                        translated = cached_translate(syn, \"en\", lang)\n",
    "                        if len(tokenizer.tokenize(translated)) == 1:\n",
    "                            valid_synonyms.append(syn)\n",
    "                if not valid_synonyms:\n",
    "                    return None\n",
    "                chosen_syn = random.choice(valid_synonyms)\n",
    "            else:\n",
    "                chosen_syn = random.choice(list(synonym_candidates))\n",
    "\n",
    "            return (\n",
    "                chosen_syn if lang == \"en\" else cached_translate(chosen_syn, \"en\", lang)\n",
    "            )\n",
    "\n",
    "        # Step 3: If we do have native synonyms, filter if tokenizer provided\n",
    "        if tokenizer:\n",
    "            filtered = [s for s in synonyms if len(tokenizer.tokenize(s)) == 1]\n",
    "            if not filtered:\n",
    "                return None\n",
    "            return random.choice(filtered)\n",
    "\n",
    "        return random.choice(synonyms)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def token_to_word(token, tokenizer):\n",
    "    return tokenizer.decode([token]).strip()\n",
    "\n",
    "\n",
    "def replace_tokens_with_synonyms(\n",
    "    inputs, tokenizer, device, lang=\"en\", replacement_prob=0.15\n",
    "):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].clone()\n",
    "\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if random.random() < replacement_prob:\n",
    "                token_id = input_ids[i, j].item()\n",
    "                word = token_to_word(token_id, tokenizer)\n",
    "\n",
    "                if (\n",
    "                    word.lower() in stop_words\n",
    "                    or word.startswith(\"##\")\n",
    "                    or not word.isalpha()\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                synonym = get_synonym(word, lang=lang, tokenizer=tokenizer)\n",
    "                if not synonym:\n",
    "                    synonym = word\n",
    "\n",
    "                synonym_tokens = tokenizer(\n",
    "                    synonym, return_tensors=\"pt\", add_special_tokens=False\n",
    "                ).to(device)\n",
    "\n",
    "                if synonym_tokens[\"input_ids\"].shape[1] == 1:\n",
    "                    if synonym_tokens[\"input_ids\"][0, 0] != token_id:\n",
    "                        input_ids[i, j] = synonym_tokens[\"input_ids\"][0, 0]\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dämpfer', 'Rückschlag', 'schallende Ohrfeige']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_german_synonyms(\"Schlappe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chair',\n",
       " 'Chief Executive',\n",
       " 'chairperson',\n",
       " 'chairwoman',\n",
       " 'chairman',\n",
       " 'United States President',\n",
       " 'prexy',\n",
       " 'President of the United States']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_omw_synonyms(\"president\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Modified: the fast brownish trick jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "modified_input_ids = replace_tokens_with_synonyms(inputs, tokenizer, device, replacement_prob=0.5)\n",
    "modified_sentence = tokenizer.decode(modified_input_ids[0])\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Modified: {modified_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_it = \"Il video mostra un gruppo di ballerini che esegue una coreografia di danza Jazz in un ambiente chiuso, probabilmente uno studio di danza.\"\n",
    "sample_zh = \"该视频展示了一群舞者在一个封闭的环境中执行爵士舞编舞，可能是一个舞蹈工作室。\"\n",
    "sample_de = \"Das Video zeigt eine Gruppe von Tänzern, die in einer geschlossenen Umgebung, wahrscheinlich einem Tanzstudio, eine Jazz-Choreografie ausführen.\"\n",
    "sample_fr = \"La vidéo montre un groupe de danseurs exécutant une chorégraphie de danse jazz dans un environnement clos, probablement un studio de danse.\"\n",
    "sample_es = \"El video muestra a un grupo de bailarines realizando una coreografía de danza jazz en un entorno cerrado, probablemente un estudio de danza.\"\n",
    "sample_pt = \"O vídeo mostra um grupo de dançarinos executando uma coreografia de dança jazz em um ambiente fechado, provavelmente um estúdio de dança.\"\n",
    "sample_ru = \"В видео показана группа танцоров, исполняющих джазовую хореографию в закрытом помещении, вероятно, в танцевальной студии.\"\n",
    "sample_ko = \"이 비디오는 아마도 댄스 스튜디오에서 닫힌 환경에서 재즈 댄스 안무를 수행하는 무용수 그룹을 보여줍니다.\"\n",
    "sample_en = \"The video shows a group of dancers performing a Jazz dance choreography in an enclosed environment, probably a dance studio.\"\n",
    "\n",
    "lang_samples = {\n",
    "    \"it\": sample_it,\n",
    "    \"zh\": sample_zh,\n",
    "    \"de\": sample_de,\n",
    "    \"fr\": sample_fr,\n",
    "    \"es\": sample_es,\n",
    "    \"pt\": sample_pt,\n",
    "    \"ru\": sample_ru,\n",
    "    \"ko\": sample_ko,\n",
    "    \"en\": sample_en,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Das Video zeigt eine Gruppe von Tänzern, die in einer geschlossenen Umgebung, wahrscheinlich einem Tanzstudio, eine Jazz-Choreografie ausführen.\n",
      "Modified: per video zeigt eine gruppe von tanzern, pro in einer geschlossenen circagebung, wahrscheinlich einem tanzstudio, eine jazz - choreografie obfuhren.\n"
     ]
    }
   ],
   "source": [
    "lang_choice = \"de\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    lang_samples[lang_choice],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").to(device)\n",
    "\n",
    "modified_input_ids = replace_tokens_with_synonyms(inputs, tokenizer, device, lang=lang_choice, replacement_prob=1)\n",
    "modified_sentence = tokenizer.decode(modified_input_ids[0])\n",
    "print(f\"Original: {lang_samples[lang_choice]}\")\n",
    "print(f\"Modified: {modified_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
